{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hmasson/anaconda3/envs/project/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/hmasson/anaconda3/envs/project/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Load the GPT2 model and resize its token embeddings to accommodate the new padding token\n",
    "generator_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "generator_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "discriminator_model = GPT2LMHeadModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "discriminator_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = generator_model\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = discriminator_model\n",
    "        self.classifier = nn.Linear(self.model.config.n_embd, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.hidden_states[-1]  # Get the last hidden state\n",
    "        cls_token = hidden_states[:, 0, :]  # Get the embeddings of the [CLS] token\n",
    "        return self.classifier(cls_token)\n",
    "\n",
    "# Instantiate models\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Define loss functions\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Define optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=2e-5)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=2e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load your dataset\n",
    "dataset = load_dataset('json', data_files='/home/hmasson/deepL/data/original_data.jsonl')['train']\n",
    "\n",
    "# Initialize the tokenizer\n",
    "# model_name = \"bert-base-uncased\"\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['review'], padding='max_length', truncation=True, max_length=64)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Convert tokenized dataset to PyTorch tensors\n",
    "input_ids = torch.tensor(tokenized_dataset['input_ids'])\n",
    "attention_masks = torch.tensor(tokenized_dataset['attention_mask'])\n",
    "labels = torch.tensor(tokenized_dataset['label'])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n",
    "    input_ids, labels, random_state=42, test_size=0.1\n",
    ")\n",
    "train_masks, validation_masks, _, _ = train_test_split(\n",
    "    attention_masks, labels, random_state=42, test_size=0.1\n",
    ")\n",
    "\n",
    "# Create TensorDataset\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "validation_dataset = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=32)\n",
    "validation_dataloader = DataLoader(validation_dataset, sampler=SequentialSampler(validation_dataset), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "N_EPOCHS = 15\n",
    "best_loss_G = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    total_loss_G = 0\n",
    "    total_loss_D = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_masks, labels = batch\n",
    "\n",
    "        # Training Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        real_labels = torch.ones((input_ids.size(0), 1), device=input_ids.device)\n",
    "        fake_labels = torch.zeros((input_ids.size(0), 1), device=input_ids.device)\n",
    "\n",
    "        outputs = discriminator(input_ids, attention_mask=attention_masks)\n",
    "        loss_real = criterion(outputs, real_labels)\n",
    "        \n",
    "        # Generate fake text\n",
    "        noise = torch.randint(0, tokenizer.vocab_size, input_ids.shape, device=input_ids.device)\n",
    "        fake_logits = generator(noise)\n",
    "        fake_texts = torch.argmax(fake_logits, dim=-1).type(torch.LongTensor).to(input_ids.device)\n",
    "        fake_outputs = discriminator(fake_texts.detach())\n",
    "        loss_fake = criterion(fake_outputs, fake_labels)\n",
    "        \n",
    "        loss_D = (loss_real + loss_fake) / 2\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Training Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        fake_outputs = discriminator(fake_texts)\n",
    "        loss_G = criterion(fake_outputs, real_labels)\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        total_loss_G += loss_G.item()\n",
    "        total_loss_D += loss_D.item()\n",
    "    \n",
    "    avg_loss_G = total_loss_G / len(train_dataloader)\n",
    "    avg_loss_D = total_loss_D / len(train_dataloader)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{N_EPOCHS} | Generator Loss: {avg_loss_G} | Discriminator Loss: {avg_loss_D}')\n",
    "    \n",
    "    # Save the best generator model\n",
    "    if avg_loss_G < best_loss_G:\n",
    "        best_loss_G = avg_loss_G\n",
    "        torch.save(generator.state_dict(), 'best-generator-model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the generator model\n",
    "generator = Generator()\n",
    "\n",
    "# Load the best model state\n",
    "generator.load_state_dict(torch.load('best-generator-model.pt'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "generator.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to decode generated text tokens\n",
    "def decode_generated_text(generated_text_tokens):\n",
    "    decoded_texts = []\n",
    "    for tokens in generated_text_tokens:\n",
    "        decoded_text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "        decoded_texts.append(decoded_text)\n",
    "    return decoded_texts\n",
    "\n",
    "# Generate text from random noise\n",
    "def generate_text(generator, tokenizer, num_samples=5, max_length=64):\n",
    "    # Generate random noise as input\n",
    "    noise = torch.randint(0, tokenizer.vocab_size, (num_samples, max_length), device='cuda')\n",
    "    \n",
    "    # Generate logits from the noise\n",
    "    with torch.no_grad():\n",
    "        generated_logits = generator(noise)\n",
    "    \n",
    "    # Convert logits to token indices\n",
    "    generated_text_tokens = torch.argmax(generated_logits, dim=-1)\n",
    "    \n",
    "    # Decode the generated text tokens\n",
    "    generated_texts = decode_generated_text(generated_text_tokens)\n",
    "    \n",
    "    return generated_texts\n",
    "\n",
    "# Generate and print text samples\n",
    "generated_texts = generate_text(generator, tokenizer, num_samples=5)\n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(f\"Generated Text {i+1}:\\n{text}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
